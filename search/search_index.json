{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ReconST","text":"<p>Gene panel selection for spatial transcriptomics.</p>"},{"location":"#overview","title":"Overview","text":"<p>ReconST provides: - FeatureScreeningAutoencoder - Gene selection pipeline - Training &amp; evaluation utilities</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"#links","title":"Links","text":"<p>GitHub: https://github.com/haoranlustat/ReconST</p>"},{"location":"api/","title":"API Reference","text":"<p>ReconST: Gene panel selection for spatial transcriptomics</p>"},{"location":"api/#reconst.FeatureScreeningAutoencoder","title":"<code>FeatureScreeningAutoencoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Autoencoder with learnable feature importance weights</p> Source code in <code>reconst/model.py</code> <pre><code>class FeatureScreeningAutoencoder(nn.Module):\n    \"\"\"Autoencoder with learnable feature importance weights\"\"\"\n\n    def __init__(self, input_size, embedding_size, dp=0.2, lk=0.2):\n        super(FeatureScreeningAutoencoder, self).__init__()\n\n        self.feature_importance = nn.Parameter(torch.ones(input_size))\n\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 512),\n            nn.LeakyReLU(lk, inplace=True),\n            nn.Dropout(dp),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(lk, inplace=True),\n            nn.Dropout(dp),\n            nn.Linear(256, embedding_size),\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(embedding_size, 256),\n            nn.LeakyReLU(lk, inplace=True),\n            nn.Dropout(dp),\n            nn.Linear(256, 512),\n            nn.LeakyReLU(lk, inplace=True),\n            nn.Linear(512, input_size),\n        )\n\n    def forward(self, x):\n        screened_features = x * self.feature_importance\n        encoded = self.encoder(screened_features)\n        decoded = self.decoder(encoded)\n        return screened_features, encoded, decoded\n</code></pre>"},{"location":"api/#reconst.create_data_loader","title":"<code>create_data_loader(adata, batch_size=256, train_split=0.8, shuffle=True)</code>","text":"<p>Create train and test data loaders from AnnData object</p> Source code in <code>reconst/data.py</code> <pre><code>def create_data_loader(adata, batch_size=256, train_split=0.8, shuffle=True):\n    \"\"\"Create train and test data loaders from AnnData object\"\"\"\n    matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n    gene_matrix = torch.tensor(matrix, dtype=torch.float32)\n    dataset = GeneExpressionDataset(gene_matrix)\n\n    train_size = int(train_split * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, test_loader\n</code></pre>"},{"location":"api/#reconst.evaluate_model","title":"<code>evaluate_model(model, test_loader, gene_mask=None, device='cpu')</code>","text":"<p>Evaluate model on test data, optionally with gene filtering</p> Source code in <code>reconst/trainer.py</code> <pre><code>def evaluate_model(model, test_loader, gene_mask=None, device='cpu'):\n    \"\"\"Evaluate model on test data, optionally with gene filtering\"\"\"\n    criterion = nn.MSELoss()\n    model.eval()\n    test_loss = 0\n\n    with torch.no_grad():\n        for data in test_loader:\n            genes = data.to(device)\n\n            if gene_mask is not None:\n                if not isinstance(gene_mask, torch.Tensor):\n                    gene_mask = torch.tensor(gene_mask)\n                gene_mask = gene_mask.to(device)\n                filtered_genes = genes * gene_mask\n            else:\n                filtered_genes = genes\n\n            _, _, outputs = model(filtered_genes)\n            loss = criterion(outputs, genes)\n            test_loss += loss.item() * genes.size(0)\n\n    test_loss /= len(test_loader.dataset)\n    return test_loss\n</code></pre>"},{"location":"api/#reconst.prepare_common_genes","title":"<code>prepare_common_genes(adata1, adata2)</code>","text":"<p>Find common genes between two datasets and filter both</p> Source code in <code>reconst/data.py</code> <pre><code>def prepare_common_genes(adata1, adata2):\n    \"\"\"Find common genes between two datasets and filter both\"\"\"\n    genes1 = set(adata1.var_names)\n    genes2 = set(adata2.var_names)\n    common_genes = list(genes1.intersection(genes2))\n\n    genes_to_keep1 = [gene in common_genes for gene in adata1.var_names]\n    genes_to_keep2 = [gene in common_genes for gene in adata2.var_names]\n\n    adata1_common = adata1[:, genes_to_keep1].copy()\n    adata2_common = adata2[:, genes_to_keep2].copy()\n\n    # Align gene order\n    adata2_common = adata2_common[:, adata1_common.var_names].copy()\n\n    return adata1_common, adata2_common, common_genes\n</code></pre>"},{"location":"api/#reconst.select_genes","title":"<code>select_genes(model, threshold=0.001)</code>","text":"<p>Select genes based on feature importance threshold</p> Source code in <code>reconst/trainer.py</code> <pre><code>def select_genes(model, threshold=0.001):\n    \"\"\"Select genes based on feature importance threshold\"\"\"\n    feature_importances = model.feature_importance.data.cpu().numpy()\n    genes_mask = feature_importances &gt;= threshold\n    return genes_mask, feature_importances\n</code></pre>"},{"location":"api/#reconst.train_model","title":"<code>train_model(model, train_loader, test_loader, num_epochs=20, lr=0.001, weight_decay=1e-05, l_lambda=0.0001, device='cpu')</code>","text":"<p>Train the autoencoder model</p> Source code in <code>reconst/trainer.py</code> <pre><code>def train_model(model, train_loader, test_loader, num_epochs=20, lr=1e-3,\n                weight_decay=1e-5, l_lambda=1e-4, device='cpu'):\n    \"\"\"Train the autoencoder model\"\"\"\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    train_losses = []\n    test_losses = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n\n        for data in train_loader:\n            genes = data.to(device)\n            screened_features, _, outputs = model(genes)\n\n            loss = criterion(outputs, genes)\n            l1_penalty = l_lambda * torch.norm(model.feature_importance, p=1)\n            total_loss = loss + l1_penalty\n\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * genes.size(0)\n\n        train_loss /= len(train_loader.dataset)\n        train_losses.append(train_loss)\n\n        # Evaluate on test data\n        model.eval()\n        test_loss = 0\n        with torch.no_grad():\n            for data in test_loader:\n                genes = data.to(device)\n                _, _, outputs = model(genes)\n                loss = criterion(outputs, genes)\n                test_loss += loss.item() * genes.size(0)\n\n        test_loss /= len(test_loader.dataset)\n        test_losses.append(test_loss)\n\n        print(f'Epoch [{epoch+1}/{num_epochs}], Train: {train_loss:.4f}, Test: {test_loss:.4f}')\n\n    return train_losses, test_losses\n</code></pre>"},{"location":"usage/","title":"Basic usage","text":"<pre><code>from reconst import prepare_common_genes, train_model, select_genes\n</code></pre> <p>(details later)</p>"}]}